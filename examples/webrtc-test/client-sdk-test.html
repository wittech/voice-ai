<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Rapida Voice AI - Native WebRTC Transport Test</title>
    <style>
      * {
        box-sizing: border-box;
      }
      body {
        font-family:
          -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
          "Helvetica Neue", Arial, sans-serif;
        max-width: 800px;
        margin: 0 auto;
        padding: 40px 20px;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        min-height: 100vh;
      }
      .container {
        background: white;
        border-radius: 16px;
        padding: 32px;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
      }
      h1 {
        margin-top: 0;
        color: #333;
        font-size: 24px;
        text-align: center;
      }
      .transport-mode {
        background: #e8f5e9;
        border: 1px solid #4caf50;
        border-radius: 8px;
        padding: 16px;
        margin-bottom: 24px;
        text-align: center;
      }
      .transport-mode h3 {
        margin: 0 0 8px;
        color: #2e7d32;
      }
      .transport-mode p {
        margin: 0;
        color: #558b2f;
        font-size: 14px;
      }
      .status {
        text-align: center;
        padding: 16px;
        border-radius: 8px;
        margin-bottom: 24px;
        font-weight: 500;
      }
      .status.disconnected {
        background: #ffebee;
        color: #c62828;
      }
      .status.connecting {
        background: #fff3e0;
        color: #e65100;
      }
      .status.connected {
        background: #e8f5e9;
        color: #2e7d32;
      }
      .controls {
        display: flex;
        gap: 16px;
        justify-content: center;
        flex-wrap: wrap;
        margin-bottom: 24px;
      }
      button {
        padding: 12px 24px;
        font-size: 16px;
        border: none;
        border-radius: 8px;
        cursor: pointer;
        transition: all 0.2s;
        font-weight: 500;
      }
      button:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
      }
      button:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }
      #connectBtn {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
      }
      #disconnectBtn {
        background: #f44336;
        color: white;
      }
      #muteBtn {
        background: #ff9800;
        color: white;
      }
      .visualizer-container {
        display: flex;
        gap: 24px;
        margin-bottom: 24px;
      }
      .visualizer {
        flex: 1;
        text-align: center;
      }
      .visualizer h4 {
        margin: 0 0 8px;
        color: #666;
        font-size: 14px;
      }
      canvas {
        width: 100%;
        height: 80px;
        background: #f5f5f5;
        border-radius: 8px;
      }
      .config {
        background: #f5f5f5;
        border-radius: 8px;
        padding: 16px;
        margin-bottom: 24px;
      }
      .config h3 {
        margin: 0 0 12px;
        color: #333;
        font-size: 14px;
      }
      .config-row {
        display: flex;
        gap: 16px;
        margin-bottom: 12px;
      }
      .config-row label {
        display: flex;
        flex-direction: column;
        gap: 4px;
        flex: 1;
        font-size: 12px;
        color: #666;
      }
      .config-row input,
      .config-row select {
        padding: 8px;
        border: 1px solid #ddd;
        border-radius: 4px;
        font-size: 14px;
      }
      .logs {
        background: #263238;
        border-radius: 8px;
        padding: 16px;
        height: 200px;
        overflow-y: auto;
        font-family: "Monaco", "Menlo", monospace;
        font-size: 12px;
      }
      .logs .log {
        margin-bottom: 4px;
      }
      .logs .log.info {
        color: #4fc3f7;
      }
      .logs .log.success {
        color: #81c784;
      }
      .logs .log.error {
        color: #e57373;
      }
      .logs .log.warn {
        color: #ffb74d;
      }
      .comparison {
        background: #f3e5f5;
        border-radius: 8px;
        padding: 16px;
        margin-bottom: 24px;
      }
      .comparison h3 {
        margin: 0 0 12px;
        color: #7b1fa2;
        font-size: 14px;
      }
      .comparison table {
        width: 100%;
        border-collapse: collapse;
        font-size: 12px;
      }
      .comparison th,
      .comparison td {
        padding: 8px;
        text-align: left;
        border-bottom: 1px solid #e1bee7;
      }
      .comparison th {
        color: #7b1fa2;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>üéôÔ∏è Rapida Voice AI - Native WebRTC Transport</h1>

      <div class="transport-mode">
        <h3>‚ú® Native WebRTC Transport Mode</h3>
        <p>
          Audio flows directly through WebRTC media tracks (SRTP) - NOT through
          gRPC!
        </p>
      </div>

      <div class="comparison">
        <h3>Transport Comparison</h3>
        <table>
          <tr>
            <th>Feature</th>
            <th>gRPC Transport</th>
            <th>Native WebRTC</th>
          </tr>
          <tr>
            <td>Audio Path</td>
            <td>AudioWorklet ‚Üí gRPC stream</td>
            <td>RTCPeerConnection media tracks</td>
          </tr>
          <tr>
            <td>Signaling</td>
            <td>gRPC bidirectional</td>
            <td>WebSocket (SDP/ICE only)</td>
          </tr>
          <tr>
            <td>Latency</td>
            <td>Medium</td>
            <td>Low (direct peer)</td>
          </tr>
          <tr>
            <td>NAT Traversal</td>
            <td>Via proxy</td>
            <td>ICE with STUN/TURN</td>
          </tr>
        </table>
      </div>

      <div id="status" class="status disconnected">Disconnected</div>

      <div class="config">
        <h3>Configuration</h3>
        <div class="config-row">
          <label>
            Assistant API Base URL
            <input type="text" id="apiBaseUrl" value="ws://localhost:9007" />
          </label>
          <label>
            Assistant ID
            <input type="text" id="assistantId" value="1" />
          </label>
        </div>
        <div class="config-row">
          <label>
            Sample Rate
            <select id="sampleRate">
              <option value="8000">8000 Hz</option>
              <option value="16000" selected>16000 Hz</option>
              <option value="24000">24000 Hz</option>
              <option value="48000">48000 Hz</option>
            </select>
          </label>
          <label>
            Input Device
            <select id="inputDevice">
              <option value="">Default</option>
            </select>
          </label>
        </div>
      </div>

      <div class="controls">
        <button id="connectBtn">üîå Connect</button>
        <button id="disconnectBtn" disabled>‚ùå Disconnect</button>
        <button id="muteBtn" disabled>üîá Mute</button>
      </div>

      <div class="visualizer-container">
        <div class="visualizer">
          <h4>Input (Microphone)</h4>
          <canvas id="inputCanvas"></canvas>
        </div>
        <div class="visualizer">
          <h4>Output (Speaker)</h4>
          <canvas id="outputCanvas"></canvas>
        </div>
      </div>

      <div class="logs" id="logs"></div>
    </div>

    <script type="module">
      // This example shows how the SDK would be used
      // In production, you'd import from the Rapida SDK:
      // import { VoiceAgent, InputOptions, OutputOptions, AgentConfig, ConnectionConfig, AudioTransportMode } from '@anthropic/rapida';

      const statusEl = document.getElementById("status");
      const logsEl = document.getElementById("logs");
      const connectBtn = document.getElementById("connectBtn");
      const disconnectBtn = document.getElementById("disconnectBtn");
      const muteBtn = document.getElementById("muteBtn");
      const apiBaseUrlInput = document.getElementById("apiBaseUrl");
      const assistantIdInput = document.getElementById("assistantId");
      const sampleRateSelect = document.getElementById("sampleRate");
      const inputDeviceSelect = document.getElementById("inputDevice");
      const inputCanvas = document.getElementById("inputCanvas");
      const outputCanvas = document.getElementById("outputCanvas");

      // Session info from server
      let sessionId = null;

      let peerConnection = null;
      let signalingSocket = null;
      let localStream = null;
      let isMuted = false;
      let animationId = null;

      // Setup canvas contexts
      const inputCtx = inputCanvas.getContext("2d");
      const outputCtx = outputCanvas.getContext("2d");
      inputCanvas.width = inputCanvas.offsetWidth;
      inputCanvas.height = inputCanvas.offsetHeight;
      outputCanvas.width = outputCanvas.offsetWidth;
      outputCanvas.height = outputCanvas.offsetHeight;

      // Audio analysers
      let audioContext = null;
      let inputAnalyser = null;
      let outputAnalyser = null;

      function log(message, type = "info") {
        const logEl = document.createElement("div");
        logEl.className = `log ${type}`;
        logEl.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
        logsEl.appendChild(logEl);
        logsEl.scrollTop = logsEl.scrollHeight;
        console.log(message);
      }

      function setStatus(status, state) {
        statusEl.textContent = status;
        statusEl.className = `status ${state}`;
      }

      // Enumerate audio devices
      async function enumerateDevices() {
        try {
          const devices = await navigator.mediaDevices.enumerateDevices();
          const audioInputs = devices.filter((d) => d.kind === "audioinput");
          inputDeviceSelect.innerHTML = '<option value="">Default</option>';
          audioInputs.forEach((device) => {
            const option = document.createElement("option");
            option.value = device.deviceId;
            option.textContent =
              device.label || `Microphone ${inputDeviceSelect.options.length}`;
            inputDeviceSelect.appendChild(option);
          });
        } catch (err) {
          log("Failed to enumerate devices: " + err.message, "error");
        }
      }

      async function connect() {
        try {
          setStatus("Connecting...", "connecting");
          log("Starting native WebRTC connection...");

          const sampleRate = parseInt(sampleRateSelect.value);
          const deviceId = inputDeviceSelect.value;

          // 1. Create audio context
          audioContext = new AudioContext({ sampleRate });

          // 2. Get microphone access
          const constraints = {
            audio: {
              sampleRate: { ideal: sampleRate },
              channelCount: { ideal: 1 },
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
            video: false,
          };
          if (deviceId) {
            constraints.audio.deviceId = { exact: deviceId };
          }

          log("Requesting microphone access...");
          localStream = await navigator.mediaDevices.getUserMedia(constraints);
          log("Microphone access granted", "success");

          // Setup input analyser
          const inputSource = audioContext.createMediaStreamSource(localStream);
          inputAnalyser = audioContext.createAnalyser();
          inputAnalyser.fftSize = 256;
          inputSource.connect(inputAnalyser);

          // 3. Create peer connection
          peerConnection = new RTCPeerConnection({
            iceServers: [
              { urls: "stun:stun.l.google.com:19302" },
              { urls: "stun:stun1.l.google.com:19302" },
            ],
          });

          // Add local tracks
          localStream.getTracks().forEach((track) => {
            peerConnection.addTrack(track, localStream);
          });
          log("Added local audio tracks");

          // Set codec preferences to prefer PCMU (G.711 Œº-law) for compatibility
          const transceivers = peerConnection.getTransceivers();
          transceivers.forEach((transceiver) => {
            if (transceiver.sender.track?.kind === "audio") {
              const codecs =
                RTCRtpSender.getCapabilities("audio")?.codecs || [];
              // Sort codecs to prefer PCMU > PCMA > others
              const sortedCodecs = codecs.sort((a, b) => {
                const priority = { "audio/PCMU": 0, "audio/PCMA": 1 };
                const aPriority =
                  priority[a.mimeType] !== undefined ? priority[a.mimeType] : 2;
                const bPriority =
                  priority[b.mimeType] !== undefined ? priority[b.mimeType] : 2;
                return aPriority - bPriority;
              });
              try {
                transceiver.setCodecPreferences(sortedCodecs);
                log("Codec preferences set: PCMU preferred");
              } catch (e) {
                log("Failed to set codec preferences: " + e.message, "warn");
              }
            }
          });

          // Handle remote tracks
          peerConnection.ontrack = (event) => {
            log("Remote track received: " + event.track.kind, "success");
            const remoteStream = event.streams[0];

            // Play remote audio
            const audioEl = new Audio();
            audioEl.srcObject = remoteStream;
            audioEl.autoplay = true;
            audioEl
              .play()
              .catch((e) =>
                log("Audio autoplay blocked: " + e.message, "warn"),
              );

            // Setup output analyser
            const outputSource =
              audioContext.createMediaStreamSource(remoteStream);
            outputAnalyser = audioContext.createAnalyser();
            outputAnalyser.fftSize = 256;
            outputSource.connect(outputAnalyser);
          };

          // Handle ICE candidates - send in server's expected format
          peerConnection.onicecandidate = (event) => {
            if (event.candidate) {
              const candidateJson = event.candidate.toJSON();
              log("Sending ICE candidate");
              signalingSocket.send(
                JSON.stringify({
                  type: "ice_candidate",
                  sessionId: sessionId,
                  candidate: {
                    candidate: candidateJson.candidate,
                    sdpMid: candidateJson.sdpMid,
                    sdpMLineIndex: candidateJson.sdpMLineIndex,
                    usernameFragment: candidateJson.usernameFragment,
                  },
                }),
              );
            }
          };

          // Handle connection state
          peerConnection.onconnectionstatechange = () => {
            log("Connection state: " + peerConnection.connectionState);
            if (peerConnection.connectionState === "connected") {
              setStatus("Connected via Native WebRTC", "connected");
              log("WebRTC peer connection established!", "success");
            } else if (
              peerConnection.connectionState === "disconnected" ||
              peerConnection.connectionState === "failed"
            ) {
              setStatus("Disconnected", "disconnected");
            }
          };

          // 4. Connect signaling WebSocket to assistant-api
          const baseUrl = apiBaseUrlInput.value;
          const assistantId = assistantIdInput.value;
          const signalingUrl = `${baseUrl}/v1/talk/webrtc/${assistantId}`;
          log("Connecting to assistant-api WebRTC: " + signalingUrl);

          signalingSocket = new WebSocket(signalingUrl);

          signalingSocket.onopen = async () => {
            log("Signaling WebSocket connected", "success");

            // Send connect message - server will respond with config and offer
            log("Sending connect request...");
            signalingSocket.send(JSON.stringify({ type: "connect" }));
          };

          signalingSocket.onmessage = async (event) => {
            const msg = JSON.parse(event.data);
            log("Received signaling: " + msg.type);

            switch (msg.type) {
              case "config":
                // Server sends config with ICE servers
                sessionId = msg.sessionId;
                log("Session ID: " + sessionId, "success");
                if (msg.metadata?.ice_servers) {
                  log("Received ICE servers config");
                }
                break;

              case "offer":
                // Server sends offer - create and send answer
                if (msg.sdp) {
                  log("Received SDP offer from server");
                  await peerConnection.setRemoteDescription({
                    type: "offer",
                    sdp: msg.sdp,
                  });

                  const answer = await peerConnection.createAnswer();
                  await peerConnection.setLocalDescription(answer);

                  log("Sending SDP answer");
                  signalingSocket.send(
                    JSON.stringify({
                      type: "answer",
                      sessionId: sessionId,
                      sdp: answer.sdp,
                    }),
                  );
                }
                break;

              case "answer":
                // Fallback if we sent offer first
                if (msg.sdp) {
                  await peerConnection.setRemoteDescription({
                    type: "answer",
                    sdp: msg.sdp,
                  });
                  log("Remote description set", "success");
                }
                break;

              case "ice_candidate":
                if (msg.candidate) {
                  await peerConnection.addIceCandidate({
                    candidate: msg.candidate.candidate,
                    sdpMid: msg.candidate.sdpMid,
                    sdpMLineIndex: msg.candidate.sdpMLineIndex,
                  });
                  log("Added remote ICE candidate");
                }
                break;

              case "disconnect":
                log("Server requested disconnect", "warn");
                await disconnect();
                break;

              case "error":
                log("Server error: " + msg.error, "error");
                break;
            }
          };

          signalingSocket.onerror = (error) => {
            log("Signaling error: " + error, "error");
            setStatus("Connection Error", "disconnected");
          };

          signalingSocket.onclose = () => {
            log("Signaling WebSocket closed");
          };

          // Update UI
          connectBtn.disabled = true;
          disconnectBtn.disabled = false;
          muteBtn.disabled = false;

          // Start visualization
          startVisualization();
        } catch (err) {
          log("Connection error: " + err.message, "error");
          setStatus("Connection Failed", "disconnected");
          await disconnect();
        }
      }

      async function disconnect() {
        log("Disconnecting...");

        // Send disconnect message to server
        if (signalingSocket && signalingSocket.readyState === WebSocket.OPEN) {
          signalingSocket.send(
            JSON.stringify({
              type: "disconnect",
              sessionId: sessionId,
            }),
          );
        }

        // Stop visualization
        if (animationId) {
          cancelAnimationFrame(animationId);
          animationId = null;
        }

        // Stop local tracks
        if (localStream) {
          localStream.getTracks().forEach((track) => track.stop());
          localStream = null;
        }

        // Close peer connection
        if (peerConnection) {
          peerConnection.close();
          peerConnection = null;
        }

        // Close signaling socket
        if (signalingSocket) {
          signalingSocket.close();
          signalingSocket = null;
        }

        // Close audio context
        if (audioContext && audioContext.state !== "closed") {
          await audioContext.close();
        }
        audioContext = null;
        inputAnalyser = null;
        outputAnalyser = null;
        sessionId = null;

        // Update UI
        setStatus("Disconnected", "disconnected");
        connectBtn.disabled = false;
        disconnectBtn.disabled = true;
        muteBtn.disabled = true;
        muteBtn.textContent = "üîá Mute";
        isMuted = false;

        // Clear canvases
        inputCtx.clearRect(0, 0, inputCanvas.width, inputCanvas.height);
        outputCtx.clearRect(0, 0, outputCanvas.width, outputCanvas.height);

        log("Disconnected", "success");
      }

      function toggleMute() {
        isMuted = !isMuted;
        if (localStream) {
          localStream.getAudioTracks().forEach((track) => {
            track.enabled = !isMuted;
          });
        }
        muteBtn.textContent = isMuted ? "üîä Unmute" : "üîá Mute";
        log(isMuted ? "Microphone muted" : "Microphone unmuted");
      }

      function drawVisualizer(analyser, ctx, canvas, color) {
        if (!analyser) {
          ctx.fillStyle = "#f5f5f5";
          ctx.fillRect(0, 0, canvas.width, canvas.height);
          return;
        }

        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        analyser.getByteFrequencyData(dataArray);

        ctx.fillStyle = "#f5f5f5";
        ctx.fillRect(0, 0, canvas.width, canvas.height);

        const barWidth = (canvas.width / bufferLength) * 2.5;
        let x = 0;

        for (let i = 0; i < bufferLength; i++) {
          const barHeight = (dataArray[i] / 255) * canvas.height;

          ctx.fillStyle = color;
          ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);

          x += barWidth + 1;
        }
      }

      function startVisualization() {
        function draw() {
          drawVisualizer(inputAnalyser, inputCtx, inputCanvas, "#667eea");
          drawVisualizer(outputAnalyser, outputCtx, outputCanvas, "#4caf50");
          animationId = requestAnimationFrame(draw);
        }
        draw();
      }

      // Event listeners
      connectBtn.addEventListener("click", connect);
      disconnectBtn.addEventListener("click", disconnect);
      muteBtn.addEventListener("click", toggleMute);

      // Initial setup
      enumerateDevices();
      log("Ready. Click Connect to start native WebRTC transport.");
      log(
        "This bypasses gRPC - audio flows directly through WebRTC peer connection!",
      );
    </script>
  </body>
</html>
