package internal_azure_callers

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"
	"time"

	"github.com/openai/openai-go"
	"github.com/openai/openai-go/shared"

	internal_callers "github.com/rapidaai/api/integration-api/internal/caller"
	internal_caller_metrics "github.com/rapidaai/api/integration-api/internal/caller/metrics"
	"github.com/rapidaai/pkg/commons"
	"github.com/rapidaai/pkg/utils"
	"github.com/rapidaai/protos"
)

type largeLanguageCaller struct {
	AzureAi
}

func NewLargeLanguageCaller(logger commons.Logger, credential *protos.Credential) internal_callers.LargeLanguageCaller {
	return &largeLanguageCaller{
		AzureAi: azure(logger, credential),
	}
}

func (llc *largeLanguageCaller) GetChatCompletion(
	ctx context.Context,
	allMessages []*protos.Message,
	options *internal_callers.ChatCompletionOptions,
) (*protos.Message, []*protos.Metric, error) {
	metrics := internal_caller_metrics.NewMetricBuilder(options.RequestId)
	metrics.OnStart()

	client, err := llc.GetClient()
	if err != nil {
		llc.logger.Errorf("chat completion unable to get client for openai %v", err)
		return nil, metrics.OnFailure().Build(), err
	}

	// message and options
	llmRequest := llc.getChatCompleteParameter(options)
	llmRequest.Messages = llc.buildHistory(allMessages)

	// prehook
	options.PreHook(utils.ToJson(llmRequest))

	// chat complitions
	resp, err := client.Chat.Completions.New(ctx, llmRequest)
	if err != nil {
		llc.logger.Errorf("chat completion failed to get response from openai %v", err)
		options.PostHook(map[string]interface{}{
			"error":  err,
			"result": resp,
		}, metrics.OnFailure().Build())
		return nil, metrics.OnFailure().Build(), err
	}

	assistantMsg := &protos.AssistantMessage{
		Contents:  make([]string, 0),
		ToolCalls: make([]*protos.ToolCall, 0),
	}
	metrics.OnSuccess()

	for _, choice := range resp.Choices {
		switch choice.FinishReason {
		case "length", "content_filter":
		case "stop":
			assistantMsg.Contents = append(assistantMsg.Contents, choice.Message.Content)
		case "function_call", "tool_calls":
			if choice.Message.ToolCalls != nil {
				for _, tool := range choice.Message.ToolCalls {
					if tool.Type == "function" {
						assistantMsg.ToolCalls = append(assistantMsg.ToolCalls, &protos.ToolCall{
							Id:   tool.ID,
							Type: string(tool.Type),
							Function: &protos.FunctionCall{
								Name:      tool.Function.Name,
								Arguments: tool.Function.Arguments,
							},
						})
					}
				}
			}
		}
	}

	message := &protos.Message{
		Role: "assistant",
		Message: &protos.Message_Assistant{
			Assistant: assistantMsg,
		},
	}

	// Add usage metrics from response
	metrics.OnAddMetrics(llc.GetComplitionUsages(resp.Usage)...)

	options.PostHook(map[string]interface{}{
		"result": resp,
	}, metrics.OnSuccess().Build())
	return message, metrics.Build(), nil
}

func (llc *largeLanguageCaller) StreamChatCompletion(
	ctx context.Context,
	allMessages []*protos.Message,
	options *internal_callers.ChatCompletionOptions,
	onStream func(string, *protos.Message) error,
	onMetrics func(string, *protos.Message, []*protos.Metric) error,
	onError func(string, error),
) error {
	start := time.Now()
	metrics := internal_caller_metrics.NewMetricBuilder(options.RequestId)
	metrics.OnStart()
	var firstTokenTime *time.Time

	client, err := llc.GetClient()
	if err != nil {
		llc.logger.Errorf("chat completion unable to get client for openai: %v", err)
		onError(options.Request.GetRequestId(), err)
		options.PostHook(map[string]interface{}{
			"error": err,
		}, metrics.OnFailure().Build())
		return err
	}

	completionsOptions := llc.getChatCompleteParameter(options)
	completionsOptions.Messages = llc.buildHistory(allMessages)
	options.PreHook(utils.ToJson(completionsOptions))
	llc.logger.Benchmark("azure.llm.GetChatCompletion.llmRequestPrepare", time.Since(start))

	// Get streaming response
	resp := client.Chat.Completions.NewStreaming(ctx, completionsOptions)
	if resp.Err() != nil {
		llc.logger.Errorf("Failed to get chat completions stream: %v", resp.Err())
		options.PostHook(map[string]interface{}{
			"result": utils.ToJson(resp),
			"error":  resp.Err(),
		}, metrics.Build())
		onError(options.Request.GetRequestId(), resp.Err())
		return resp.Err()
	}
	defer resp.Close()
	assistantMsg := &protos.AssistantMessage{
		Contents:  make([]string, 0),
		ToolCalls: make([]*protos.ToolCall, 0),
	}
	contentBuffer := make([]string, 0) // Buffer for accumulating content per choice
	hasToolCalls := false              // Flag to track if response contains tool calls

	accumulate := openai.ChatCompletionAccumulator{}
	for resp.Next() {
		chatCompletions := resp.Current()
		accumulate.AddChunk(chatCompletions)

		if _, ok := accumulate.JustFinishedContent(); ok {
			metrics.OnAddMetrics(llc.GetComplitionUsages(accumulate.Usage)...)

			// Finalize content from buffer
			assistantMsg.Contents = contentBuffer
			protoMsg := &protos.Message{
				Role: "assistant",
				Message: &protos.Message_Assistant{
					Assistant: assistantMsg,
				},
			}
			// Stream tokens only if no tool calls in response
			if !hasToolCalls {
				for _, content := range contentBuffer {
					if content != "" {
						// Record first token received time
						if firstTokenTime == nil {
							now := time.Now()
							firstTokenTime = &now
						}
						tokenMsg := &protos.Message{
							Role: "assistant",
							Message: &protos.Message_Assistant{
								Assistant: &protos.AssistantMessage{
									Contents: []string{content},
								},
							},
						}
						if err := onStream(options.Request.GetRequestId(), tokenMsg); err != nil {
							llc.logger.Warnf("error streaming token: %v", err)
						}
					}
				}
			}
			// Add first token time metric if tokens were streamed
			if firstTokenTime != nil {
				metrics.OnAddMetrics(&protos.Metric{
					Name:        "FIRST_TOKEN_RECIEVED_TIME",
					Value:       fmt.Sprintf("%d", firstTokenTime.Sub(start)),
					Description: "Time to receive first token from LLM",
				})
			}
			// Update time taken and status
			metrics.OnSuccess()
			// Send metrics with complete message
			onMetrics(options.Request.GetRequestId(), protoMsg, metrics.Build())

			// Call PostHook after metrics for each message end
			options.PostHook(map[string]interface{}{
				"result": utils.ToJson(accumulate),
			}, metrics.Build())

			return nil
		}

		if tool, ok := accumulate.JustFinishedToolCall(); ok {
			hasToolCalls = true
			assistantMsg.ToolCalls = append(assistantMsg.ToolCalls, &protos.ToolCall{
				Id: tool.ID,
				Function: &protos.FunctionCall{
					Name:      tool.Name,
					Arguments: tool.Arguments,
				},
			})
			metrics.OnAddMetrics(llc.GetComplitionUsages(accumulate.Usage)...)
			metrics.OnSuccess()

			// Don't stream if tool calls are present
			assistantMsg.Contents = contentBuffer
			protoMsg := &protos.Message{
				Role: "assistant",
				Message: &protos.Message_Assistant{
					Assistant: assistantMsg,
				},
			}
			onMetrics(options.Request.GetRequestId(), protoMsg, metrics.Build())

			// Call PostHook only at the end of response
			options.PostHook(map[string]interface{}{
				"result": utils.ToJson(accumulate),
			}, metrics.Build())
			return nil
		}

		// Accumulate content but don't stream yet - check if tool calls will come
		for i, choice := range chatCompletions.Choices {
			content := choice.Delta.Content
			if content != "" {
				// Accumulate content per choice index
				if len(contentBuffer) <= i {
					contentBuffer = append(contentBuffer, content)
				} else {
					contentBuffer[i] += content
				}
			}
			// Check if this chunk has tool calls
			if len(choice.Delta.ToolCalls) > 0 {
				hasToolCalls = true
			}
		}
	}

	return nil
}

func (llc *largeLanguageCaller) buildHistory(allMessages []*protos.Message) []openai.ChatCompletionMessageParamUnion {
	msg := make([]openai.ChatCompletionMessageParamUnion, 0)
	for _, cntn := range allMessages {
		switch cntn.GetRole() {
		case ChatRoleUser:
			if user := cntn.GetUser(); user != nil {
				var messageContent []openai.ChatCompletionContentPartUnionParam
				messageContent = append(messageContent, openai.ChatCompletionContentPartUnionParam{
					OfText: &openai.ChatCompletionContentPartTextParam{
						Text: user.GetContent(),
					},
				})
				msg = append(msg, openai.UserMessage(messageContent))
			}
		case ChatRoleAssistant:
			if assistant := cntn.GetAssistant(); assistant != nil {
				txtContent := strings.Join(assistant.GetContents(), "")
				toolCalls := assistant.GetToolCalls()
				assistantMessage := openai.ChatCompletionAssistantMessageParam{}
				if len(txtContent) > 0 || len(toolCalls) > 0 {
					if len(txtContent) > 0 {
						assistantMessage.Content = openai.ChatCompletionAssistantMessageParamContentUnion{
							OfString: openai.String(txtContent),
						}
					}
					if len(toolCalls) > 0 {
						fctCall := make([]openai.ChatCompletionMessageToolCallParam, 0)
						for _, ttc := range toolCalls {
							fctCall = append(fctCall, openai.ChatCompletionMessageToolCallParam{
								ID: ttc.GetId(),
								Function: openai.ChatCompletionMessageToolCallFunctionParam{
									Name:      ttc.GetFunction().GetName(),
									Arguments: ttc.GetFunction().GetArguments(),
								},
							})
						}
						assistantMessage.ToolCalls = fctCall
					}
					msg = append(msg, openai.ChatCompletionMessageParamUnion{
						OfAssistant: &assistantMessage,
					})
				}
			}

		case ChatRoleSystem:
			if system := cntn.GetSystem(); system != nil {
				txtContent := system.GetContent()
				if len(txtContent) > 0 {
					msg = append(msg, openai.SystemMessage(txtContent))
				}
			}

		case ChatRoleTool:
			if tool := cntn.GetTool(); tool != nil {
				for _, t := range tool.GetTools() {
					msg = append(msg, openai.ToolMessage(t.GetContent(), t.GetId()))
				}
			}
		}
	}
	return msg
}

func (llc *largeLanguageCaller) getChatCompleteParameter(
	opts *internal_callers.ChatCompletionOptions,
) openai.ChatCompletionNewParams {
	options := openai.ChatCompletionNewParams{}
	if len(opts.ToolDefinitions) > 0 {
		fns := make([]openai.ChatCompletionToolParam, len(opts.ToolDefinitions))
		for idx, tl := range opts.ToolDefinitions {
			switch tl.Type {
			case "function":
				fn := tl.Function
				if fn != nil {
					funcDef := openai.FunctionDefinitionParam{
						Name: fn.Name,
					}
					if fn.Description != "" {
						funcDef.Description = openai.String(fn.Description)
					}
					// Always set parameters with valid JSON schema format
					if fn.Parameters != nil {
						funcDef.Parameters = fn.Parameters.ToMap()
					} else {
						// Default empty parameters with properties field for valid schema
						funcDef.Parameters = map[string]interface{}{
							"type":       "object",
							"properties": map[string]interface{}{},
						}
					}
					fns[idx] = openai.ChatCompletionToolParam{
						Function: funcDef,
					}
				}
			}
		}
		options.Tools = fns
	}

	for key, value := range opts.ModelParameter {
		switch key {
		case "model.name":
			if modelName, err := utils.AnyToString(value); err == nil {
				options.Model = modelName
			}
		case "model.user":
			if user, err := utils.AnyToString(value); err == nil {
				options.User = openai.String(user)
			}
		case "model.reasoning_effort":
			if re, err := utils.AnyToString(value); err == nil {
				options.ReasoningEffort = shared.ReasoningEffort(re)
			}
		case "model.seed":
			if seed, err := utils.AnyToInt64(value); err == nil {
				options.Seed = openai.Int(seed)
			}
		case "model.service_tier":
			if st, err := utils.AnyToString(value); err == nil {
				options.ServiceTier = openai.ChatCompletionNewParamsServiceTier(st)
			}
		case "model.top_logprobs":
			if tl, err := utils.AnyToInt64(value); err == nil {
				options.TopLogprobs = openai.Int(tl)
			}
		case "model.metadata":
			format, _ := utils.AnyToString(value)
			var mtd map[string]string
			if err := json.Unmarshal([]byte(format), &mtd); err == nil {
				options.Metadata = shared.Metadata(mtd)
			}
		case "model.frequency_penalty":
			if fp, err := utils.AnyToFloat64(value); err == nil {
				options.FrequencyPenalty = openai.Float(fp)
			}
		case "model.temperature":
			if temp, err := utils.AnyToFloat64(value); err == nil {
				options.Temperature = openai.Float(temp)
			}
		case "model.top_p":
			if topP, err := utils.AnyToFloat64(value); err == nil {
				options.TopP = openai.Float(topP)
			}
		case "model.presence_penalty":
			if pp, err := utils.AnyToFloat64(value); err == nil {
				options.PresencePenalty = openai.Float(pp)
			}
		case "model.max_completion_tokens":
			if maxTokens, err := utils.AnyToInt64(value); err == nil {
				options.MaxCompletionTokens = openai.Int(maxTokens)
			}
		case "model.stop":
			if stopStr, err := utils.AnyToString(value); err == nil {
				for _, stopper := range strings.Split(stopStr, ",") {
					if strings.TrimSpace(stopper) != "" {
						options.Stop.OfStringArray = append(options.Stop.OfStringArray, stopper)
					}
				}
			}
		case "model.tool_choice":
			// tool choice is only valid when there are tools
			if choice, err := utils.AnyToString(value); err == nil && len(opts.ToolDefinitions) > 0 {
				switch choice {
				case "auto":
					options.ToolChoice = openai.ChatCompletionToolChoiceOptionUnionParam{
						OfAuto: openai.String("auto"),
					}
				case "required":
					options.ToolChoice = openai.ChatCompletionToolChoiceOptionUnionParam{
						OfAuto: openai.String("required"),
					}
				case "none":
					options.ToolChoice = openai.ChatCompletionToolChoiceOptionUnionParam{
						OfAuto: openai.String("none"),
					}
				default:
					options.ToolChoice = openai.ChatCompletionToolChoiceOptionUnionParam{
						OfAuto: openai.String("none"),
					}
				}
			}
		case "model.response_format":
			if format, err := utils.AnyToJSON(value); err == nil {
				if _type, ok := format["type"]; ok {
					switch _type.(string) {
					case "json_object":
						options.ResponseFormat = openai.ChatCompletionNewParamsResponseFormatUnion{
							OfJSONObject: &openai.ResponseFormatJSONObjectParam{},
						}
					case "text":
						options.ResponseFormat = openai.ChatCompletionNewParamsResponseFormatUnion{}
					case "json_schema":
						if schemaData, ok := format["json_schema"].(map[string]interface{}); ok {
							jsonSchemaParam := shared.ResponseFormatJSONSchemaJSONSchemaParam{}
							jsonData, err := json.Marshal(schemaData)
							if err == nil {
								json.Unmarshal(jsonData, &jsonSchemaParam)
							}
							options.ResponseFormat = openai.ChatCompletionNewParamsResponseFormatUnion{
								OfJSONSchema: &shared.ResponseFormatJSONSchemaParam{
									JSONSchema: jsonSchemaParam,
								},
							}
						}
					}
				}
			}
		}
	}
	return options
}
